#!/usr/bin/env python3
"""
Direct LLM Service Testing
Tests the LLM service using local models and LangChain integration
"""

import sys
import os
from datetime import datetime

# Add the parent directory to the path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def test_llm_service_direct():
    """Test the LLM service directly with local models"""
    print("üß™ Direct LLM Service Testing")
    print("=" * 50)
    
    try:
        # Import the LLM service
        from agentic_mapping_ai.llm_service import llm_service
        print("‚úÖ LLM service imported successfully")
        
        # Check what methods are available
        methods = [m for m in dir(llm_service) if not m.startswith('_')]
        print(f"üìã Available methods: {methods}")
        
        # Test if we can create a simple response without external API calls
        print("\nüîç Testing service capabilities...")
        
        # Check if the service has local model support
        if hasattr(llm_service, 'local_models'):
            print("‚úÖ Local models support detected")
        else:
            print("‚ÑπÔ∏è No local models support detected")
        
        # Check if the service has fallback mechanisms
        if hasattr(llm_service, 'fallback_llm'):
            print("‚úÖ Fallback LLM support detected")
        else:
            print("‚ÑπÔ∏è No fallback LLM support detected")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Direct test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_local_model_integration():
    """Test integration with local models"""
    print("\nüß™ Local Model Integration Testing")
    print("=" * 50)
    
    try:
        # Try to use a local model through LangChain
        from langchain.llms import HuggingFacePipeline
        from transformers import pipeline
        
        print("üì• Setting up local model...")
        
        # Create a simple text generation pipeline
        text_pipeline = pipeline("text-generation", model="gpt2", max_length=50)
        
        # Wrap with LangChain
        local_llm = HuggingFacePipeline(pipeline=text_pipeline)
        print("‚úÖ Local LLM created successfully")
        
        # Test the local model
        test_prompt = "Data mapping is"
        print(f"\nüí¨ Testing local model with: '{test_prompt}'")
        
        response = local_llm(test_prompt)
        print(f"‚úÖ Local model response: {response}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Local model integration failed: {e}")
        print("üí° This might be due to missing dependencies or insufficient memory")
        return False

def test_llm_service_with_local_fallback():
    """Test LLM service with local model fallback"""
    print("\nüß™ LLM Service with Local Fallback")
    print("=" * 50)
    
    try:
        from agentic_mapping_ai.llm_service import llm_service
        
        # Create a simple test message
        test_messages = [
            {'role': 'system', 'content': 'You are a helpful AI assistant.'},
            {'role': 'user', 'content': 'Hello, can you help me with data mapping?'}
        ]
        
        print("üí¨ Testing LLM service with local fallback...")
        
        # Try to use the service with a local model
        try:
            # This might fail if no local models are configured
            response = llm_service.call_default_llm(messages=test_messages)
            print(f"‚úÖ Service response: {response}")
            return True
        except Exception as service_error:
            print(f"‚ö†Ô∏è Service call failed: {service_error}")
            print("üí° This is expected if local models aren't configured")
            
            # Try to create a local fallback
            print("\nüîß Creating local fallback...")
            from langchain.llms import HuggingFacePipeline
            from transformers import pipeline
            
            # Simple text generation
            text_pipe = pipeline("text-generation", model="gpt2", max_length=30)
            local_llm = HuggingFacePipeline(pipeline=text_pipe)
            
            # Test local fallback
            fallback_response = local_llm("Data mapping is the process of")
            print(f"‚úÖ Local fallback response: {fallback_response}")
            
            return True
            
    except Exception as e:
        print(f"‚ùå Local fallback test failed: {e}")
        return False

def test_embeddings_local():
    """Test local embeddings"""
    print("\nüß™ Local Embeddings Testing")
    print("=" * 50)
    
    try:
        from langchain.embeddings import HuggingFaceEmbeddings
        
        # Try to load a small embedding model
        print("üì• Loading local embedding model...")
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        
        # Test embedding generation
        test_text = "Data mapping is important for ETL processes"
        embedding = embeddings.embed_query(test_text)
        
        print(f"‚úÖ Generated embedding with {len(embedding)} dimensions")
        print(f"‚úÖ First 5 values: {embedding[:5]}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Local embeddings test failed: {e}")
        return False

def test_rag_with_local_models():
    """Test RAG functionality with local models"""
    print("\nüß™ RAG with Local Models Testing")
    print("=" * 50)
    
    try:
        from langchain.embeddings import HuggingFaceEmbeddings
        from langchain.vectorstores import FAISS
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        
        # Create embeddings
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        
        # Create sample documents
        documents = [
            "Data mapping transforms data from source to target format.",
            "ETL processes extract, transform, and load data.",
            "PySpark provides distributed data processing capabilities.",
            "Data validation ensures quality and consistency."
        ]
        
        # Split documents
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=10)
        texts = text_splitter.create_documents(documents)
        
        # Create vector store
        vectorstore = FAISS.from_documents(texts, embeddings)
        print("‚úÖ Vector store created successfully")
        
        # Test search
        query = "What is data mapping?"
        results = vectorstore.similarity_search(query, k=2)
        
        print(f"‚úÖ Search results for '{query}':")
        for i, doc in enumerate(results, 1):
            print(f"  {i}. {doc.page_content}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå RAG test failed: {e}")
        return False

def run_direct_tests():
    """Run all direct LLM service tests"""
    print("üöÄ Direct LLM Service Testing Suite")
    print("=" * 60)
    print(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    results = {}
    
    # Test 1: Direct LLM service
    results['direct_service'] = test_llm_service_direct()
    
    # Test 2: Local model integration
    results['local_integration'] = test_local_model_integration()
    
    # Test 3: Service with local fallback
    results['local_fallback'] = test_llm_service_with_local_fallback()
    
    # Test 4: Local embeddings
    results['local_embeddings'] = test_embeddings_local()
    
    # Test 5: RAG with local models
    results['rag_local'] = test_rag_with_local_models()
    
    # Summary
    print("\n" + "=" * 60)
    print("üìä DIRECT TEST RESULTS")
    print("=" * 60)
    
    for test_name, result in results.items():
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{test_name.replace('_', ' ').title()}: {status}")
    
    print("\n" + "=" * 60)
    print("üéØ RECOMMENDATIONS")
    print("=" * 60)
    
    if results.get('local_integration', False):
        print("‚úÖ Local models working - you can develop without API keys!")
    
    if results.get('local_embeddings', False):
        print("‚úÖ Local embeddings working - RAG functionality available!")
    
    if results.get('rag_local', False):
        print("‚úÖ Local RAG working - document search and retrieval available!")
    
    print("\nüí° Key Benefits of Local Models:")
    print("   ‚Ä¢ No API keys required")
    print("   ‚Ä¢ Works offline")
    print("   ‚Ä¢ No rate limits")
    print("   ‚Ä¢ Full control over models")
    print("   ‚Ä¢ Cost-effective development")
    
    print("\nüöÄ Testing complete!")

if __name__ == "__main__":
    run_direct_tests()
