# ğŸš€ Agentic Excel Workflow Application

A **comprehensive AI-powered application** that orchestrates the entire data mapping workflow from Excel files to production-ready PySpark code, using multiple intelligent agents working together.

## ğŸ¯ What This Application Does

This is a **unified agentic application** that brings together all the components we've built into a single, intelligent workflow:

### ğŸ”„ **Complete Workflow Pipeline**

```
Excel File â†’ AI Agents â†’ Validation â†’ Testing â†’ Code Generation â†’ Orchestration
    â†“              â†“           â†“         â†“           â†“              â†“
ğŸ“Š Read Excel  ğŸ¤– Validate  ğŸ§ª Generate  ğŸ’» Generate  ğŸ¯ Coordinate  ğŸ“‹ Final Report
   Mappings     Metadata     Test Cases   PySpark     Workflow      & Outputs
```

### ğŸ­ **AI Agents Working Together**

1. **ğŸ“Š Excel Processor**: Intelligently reads and parses Excel mapping files
2. **ğŸ” Metadata Validator Agent**: AI-powered validation of field mappings and schemas
3. **ğŸ§ª Test Generator**: Creates comprehensive test cases for data quality and business rules
4. **ğŸ’» Code Generator Agent**: Generates production-ready PySpark transformation code
5. **ğŸ¯ Orchestrator Agent**: Coordinates all agents and manages workflow dependencies

## ğŸš€ How to Use

### **Option 1: Easy Launcher (Recommended)**
```bash
cd demo
python run_agentic_workflow.py
```

This gives you an interactive menu to:
- Select existing Excel files
- Create sample Excel files
- Enter custom file paths
- Confirm workflow execution

### **Option 2: Direct Application**
```bash
cd demo
python agentic_excel_workflow_app.py
```

### **Option 3: Programmatic Usage**
```python
from agentic_excel_workflow_app import AgenticExcelWorkflowApp
import asyncio

app = AgenticExcelWorkflowApp()
success = await app.run_complete_workflow("path/to/excel/file.xlsx")
```

## ğŸ“ Application Structure

### **Core Application Files**
- `agentic_excel_workflow_app.py` - Main application with all workflow logic
- `run_agentic_workflow.py` - Interactive launcher script
- `create_sample_excel.py` - Creates sample Excel files for testing

### **Output Directory Structure**
```
output/
â”œâ”€â”€ excel_parsed/          # Parsed Excel mappings
â”œâ”€â”€ validation_reports/     # Metadata validation results
â”œâ”€â”€ test_cases/            # Generated test scenarios
â”œâ”€â”€ generated_code/         # PySpark transformation code
â”œâ”€â”€ workflow_logs/          # Workflow execution logs
â””â”€â”€ final_reports/          # Comprehensive workflow reports
```

## ğŸ”§ Prerequisites

1. **Python Environment**: Python 3.7+ with required packages
2. **agentic_mapping_ai**: The AI framework must be installed
3. **LLM Access**: OpenAI API key or other LLM provider configured
4. **Excel Files**: Your mapping files or use the sample generator

## ğŸ“Š What the Application Processes

### **Excel File Structure Expected**
The application intelligently detects columns in your Excel files:

| Standard Column | Excel Column Patterns |
|----------------|----------------------|
| `physical_table` | "Physical Table", "Table Name", "Target Table" |
| `logical_name` | "Logical Name", "Business Name", "Description" |
| `physical_name` | "Physical Name", "Column Name", "Field Name" |
| `data_type` | "Data Type", "Type", "Field Type" |
| `source_table` | "Source Table", "Source", "From Table" |
| `source_column` | "Source Column", "Source Field", "From Column" |
| `mapping_type` | "Mapping Type", "Type", "Transformation Type" |
| `transformation_logic` | "Transformation", "Logic", "Formula" |
| `description` | "Description", "Comment", "Notes" |

### **Mapping Types Supported**
- **Direct**: Simple field-to-field mapping
- **Derived**: Calculated fields with transformation logic
- **Goldref**: Lookup-based mappings from reference tables
- **No Mapping**: Fields that don't require transformation

## ğŸ¤– AI Agent Capabilities

### **Metadata Validator Agent**
- âœ… **Intelligent Analysis**: Uses AI to understand your mapping structure
- âœ… **Banking-Specific Rules**: Validates against financial data standards
- âœ… **Automated Issue Detection**: Finds problems without manual rule definition
- âœ… **Actionable Insights**: Provides specific recommendations for fixes

### **Code Generator Agent**
- âœ… **PySpark Generation**: Creates production-ready transformation code
- âœ… **Business Logic Integration**: Incorporates your transformation rules
- âœ… **Error Handling**: Adds robust error handling and logging
- âœ… **Performance Optimization**: Optimizes for production environments

### **Test Generator**
- âœ… **Comprehensive Testing**: Covers all mapping scenarios
- âœ… **Data Quality Tests**: Validates data integrity and consistency
- âœ… **Business Rule Tests**: Ensures compliance with business logic
- âœ… **SQL Test Queries**: Ready-to-run validation queries

### **Orchestrator Agent**
- âœ… **Workflow Management**: Coordinates all agents and steps
- âœ… **Dependency Handling**: Manages task dependencies and order
- âœ… **Error Recovery**: Handles failures gracefully
- âœ… **Progress Tracking**: Monitors workflow execution

## ğŸ“ˆ Workflow Execution Steps

### **Step 1: Agent Initialization**
```
ğŸ”§ Initializing AI Agents...
âœ… Metadata Validator Agent initialized
âœ… Code Generator Agent initialized  
âœ… Orchestrator Agent initialized
ğŸ‰ All agents initialized successfully!
```

### **Step 2: Excel Processing**
```
ğŸ“Š Step 1: Processing Excel File
ğŸ“ Reading Excel file: mapping_file.xlsx
ğŸ“‹ Found sheets: ['datahub standard mapping', 'goldref']
ğŸ“Š Loaded 25 rows from 'datahub standard mapping' sheet
ğŸ¥‡ Loaded 15 gold reference rows from 'goldref' sheet
âœ… Excel file processed successfully!
```

### **Step 3: Metadata Validation**
```
ğŸ” Step 2: Metadata Validation
ğŸ¤– Metadata Validator Agent is analyzing the mappings...
âœ… Metadata validation completed!
ğŸ“Š Validation Status: VALIDATED
âœ… Metadata validation step completed!
```

### **Step 4: Test Case Generation**
```
ğŸ§ª Step 3: Test Case Generation
ğŸ¤– Generating comprehensive test cases...
ğŸ“Š Generating tests for table: ACCT_DLY
ğŸ“Š Generating tests for table: CUST_DLY
ğŸ“Š Generating tests for table: TXN_DLY
âœ… Test case generation completed!
```

### **Step 5: Code Generation**
```
ğŸ’» Step 4: Code Generation
ğŸ¤– Code Generator Agent is creating PySpark transformations...
ğŸ“Š Generating code for table: ACCT_DLY
ğŸ“Š Generating code for table: CUST_DLY
ğŸ“Š Generating code for table: TXN_DLY
âœ… Code generation completed!
```

### **Step 6: Workflow Orchestration**
```
ğŸ¯ Step 5: Workflow Orchestration
ğŸ¤– Orchestrator Agent is coordinating the workflow...
âœ… Workflow orchestration completed!
âœ… Workflow orchestration step completed!
```

### **Step 7: Final Report**
```
ğŸ“Š Generating Final Report
ğŸ’¾ Final report saved to: output/final_reports/workflow_report_20241220_143022.json

ğŸ‰ WORKFLOW EXECUTION SUMMARY
â±ï¸  Duration: 2024-12-20T14:30:22 â†’ 2024-12-20T14:35:45
ğŸ“Š Steps: 5/5 completed
ğŸ“Š Excel Processing:
   â€¢ Source: mapping_file.xlsx
   â€¢ Mappings: 25
   â€¢ Tables: 3
ğŸ¤– Agent Performance:
   â€¢ Agents: 3
   â€¢ Results: metadata_validation, test_cases, generated_code, orchestration
```

## ğŸ¯ Sample Outputs

### **Generated PySpark Code**
```python
# PySpark Transformation for ACCT_DLY
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

def transform_acct_dly(spark: SparkSession, source_df):
    """
    Transform source data for ACCT_DLY table
    """
    
    # Apply field mappings
    transformed_df = source_df.select(
        col('ACCOUNT_NUMBER').alias('acct_nbr'),
        col('BALANCE').alias('acct_bal'),
        col('CURRENCY').alias('curr_cd'),
        expr('If ACCT_TYP_CD = "01" then "Savings" else "Checking"').alias('acct_typ_desc'),
        # ... more mappings
    )
    
    return transformed_df
```

### **Generated Test Cases**
```json
{
  "table_name": "ACCT_DLY",
  "total_fields": 8,
  "test_scenarios": [
    {
      "field": "acct_nbr",
      "test_type": "direct_mapping",
      "description": "Test direct mapping from ACCOUNT_NUMBER to acct_nbr",
      "expected_result": "Value should be identical to source field ACCOUNT_NUMBER"
    }
  ],
  "data_quality_tests": [
    {
      "test_name": "null_check",
      "description": "Check for unexpected null values in required fields",
      "sql_query": "SELECT COUNT(*) FROM ACCT_DLY WHERE required_field IS NULL"
    }
  ]
}
```

## ğŸš€ Next Steps After Execution

1. **Review Generated Code**: Check `output/generated_code/` for PySpark transformations
2. **Validate Test Cases**: Review `output/test_cases/` for testing scenarios
3. **Check Validation Reports**: Review `output/validation_reports/` for any issues
4. **Deploy Code**: Use generated PySpark code in your data pipeline
5. **Run Tests**: Execute generated test cases to validate data quality

## ğŸ†˜ Troubleshooting

### **Common Issues**

**Agent Initialization Failed**
```bash
âŒ Agent initialization failed: ImportError
```
**Solution**: Ensure `agentic_mapping_ai` is properly installed and accessible

**Excel Processing Failed**
```bash
âŒ Excel processing failed: File not found
```
**Solution**: Check file path and ensure Excel file exists

**Validation Failed**
```bash
âŒ Metadata validation failed: No validation results returned
```
**Solution**: Check LLM service configuration and API keys

### **Getting Help**

1. **Run the launcher**: `python run_agentic_workflow.py`
2. **Check prerequisites**: Ensure all dependencies are installed
3. **Verify Excel files**: Ensure your mapping files are properly formatted
4. **Check LLM access**: Verify your AI service configuration

## ğŸ‰ Success Indicators

You'll know the application is working when you see:

- âœ… "All agents initialized successfully"
- âœ… "Excel file processed successfully"
- âœ… "Metadata validation step completed"
- âœ… "Test case generation completed"
- âœ… "Code generation completed"
- âœ… "Workflow orchestration step completed"
- ğŸ“Š Comprehensive final report with all outputs

## ğŸ”— Related Components

- **Metadata Validation Demo**: `hive_metadata_validation_demo.py`
- **Agent Testing**: `test_metadata_agent.py`
- **Sample Excel Creation**: `create_sample_excel.py`
- **Enhanced Main App**: `enhanced_main.py`

## ğŸš€ Why This is Powerful

- **ğŸ¯ End-to-End**: Complete workflow from Excel to production code
- **ğŸ¤– AI-Powered**: Multiple intelligent agents working together
- **ğŸ”„ Automated**: No manual intervention required
- **ğŸ“Š Comprehensive**: Covers validation, testing, and code generation
- **ğŸ—ï¸ Production-Ready**: Generates deployable PySpark code
- **ğŸ“ˆ Scalable**: Handles any number of tables and mappings

---

**Ready to see AI agents orchestrate your entire data mapping workflow? Run the application now!** ğŸš€

```bash
cd demo
python run_agentic_workflow.py
```
